{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d3c7d6-779a-437a-84b2-55cfdf0d28f9",
   "metadata": {},
   "source": [
    "# Neuronales Netz (28x28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d1ecb-511f-490b-b0f8-58fdc2c15d97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## One-Hot-Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7faeb5b-b3cc-4319-8548-a50cbc5131ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "from scipy.special import expit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e846e4ff-7980-497f-9722-5ca3326442b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6344, 28, 28, 1)\n",
      "6344\n",
      "(2720, 28, 28, 1)\n",
      "2720\n"
     ]
    }
   ],
   "source": [
    "# load numpy array from npy file\n",
    "\n",
    "# load array\n",
    "\n",
    "X_train=load('../01_Dataset/dataset_28x28/X_train.npy').astype(np.float32) * 1.0/255.0 # normalisieren\n",
    "y_train=load('../01_Dataset/dataset_28x28/y_train.npy')\n",
    "X_test=load('../01_Dataset/dataset_28x28/X_test.npy').astype(np.float32) * 1.0/255.0  # normalisieren\n",
    "y_test=load('../01_Dataset/dataset_28x28/y_test.npy')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(len(y_train))\n",
    "print(X_test.shape)\n",
    "print(len(y_test))\n",
    "\n",
    "\n",
    "oh = OneHotEncoder()\n",
    "y_train_oh = oh.fit_transform(y_train.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcbbec1d-bb5f-4243-bd98-e308307a702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6344, 28, 28, 1)\n",
      "(2720, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5365d36-d358-4551-a5cc-3a0149780202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90016671-e95c-48c6-ac36-68e8c5eba084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# label check\n",
    "i=7\n",
    "print(y_train[i])\n",
    "print(y_train_oh[i])\n",
    "plt.imshow(X_train[i],cmap='gray')\n",
    "plt.show\n",
    "# 0: innensechskant\n",
    "# 1: philips\n",
    "# 2: pozidriv\n",
    "# 3: sechskant\n",
    "# 4: torx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b7e5c3-ee69-44ff-af6d-27f3119d32b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "(2720, 784)\n",
      "[2 4 2 ... 4 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float32).reshape(-1, 784)#reshape hier wegen label test\n",
    "X_test  = X_test.astype(np.float32).reshape(-1, 784)#\n",
    "print(X_train)\n",
    "print(X_test.shape)\n",
    "y_test = y_test.astype(np.int)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf8041e5-5b8c-4193-b3f7-2dd4567eda4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20808823529411766\n",
      "0.20808823529411766\n",
      "0.20808823529411766\n",
      "0.20808823529411766\n",
      "0.20808823529411766\n",
      "0.20808823529411766\n",
      "0.20808823529411766\n",
      "0.20808823529411766\n",
      "0.20808823529411766\n",
      "0.21433823529411763\n",
      "0.26544117647058824\n",
      "0.30220588235294116\n",
      "0.30220588235294116\n",
      "0.3058823529411765\n",
      "0.31176470588235294\n",
      "0.3161764705882353\n",
      "0.3191176470588235\n",
      "0.3235294117647059\n",
      "0.32573529411764707\n",
      "0.32830882352941176\n",
      "0.33088235294117646\n",
      "0.3327205882352941\n",
      "0.3316176470588235\n",
      "0.3290441176470588\n",
      "0.32683823529411765\n",
      "0.32941176470588235\n",
      "0.3301470588235294\n",
      "0.3316176470588235\n",
      "0.3301470588235294\n",
      "0.3297794117647059\n",
      "0.3290441176470588\n",
      "0.32941176470588235\n",
      "0.3301470588235294\n",
      "0.33088235294117646\n",
      "0.33125\n",
      "0.3316176470588235\n",
      "0.3323529411764706\n",
      "0.3323529411764706\n",
      "0.33198529411764705\n",
      "0.3341911764705882\n",
      "0.3352941176470588\n",
      "0.3352941176470588\n",
      "0.33492647058823527\n",
      "0.33455882352941174\n",
      "0.3356617647058823\n",
      "0.3352941176470588\n",
      "0.3356617647058823\n",
      "0.3360294117647059\n",
      "0.33639705882352944\n",
      "0.3375\n",
      "0.3375\n",
      "0.33933823529411766\n",
      "0.33933823529411766\n",
      "0.34044117647058825\n",
      "0.3397058823529412\n",
      "0.3386029411764706\n",
      "0.3386029411764706\n",
      "0.3360294117647059\n",
      "0.33492647058823527\n",
      "0.3352941176470588\n",
      "0.33455882352941174\n",
      "0.3341911764705882\n",
      "0.33308823529411763\n",
      "0.33308823529411763\n",
      "0.33308823529411763\n",
      "0.3338235294117647\n",
      "0.33455882352941174\n",
      "0.3352941176470588\n",
      "0.3352941176470588\n",
      "0.3356617647058823\n",
      "0.33639705882352944\n",
      "0.33639705882352944\n",
      "0.33676470588235297\n",
      "0.3375\n",
      "0.33786764705882355\n",
      "0.3375\n",
      "0.3371323529411765\n",
      "0.3382352941176471\n",
      "0.3386029411764706\n",
      "0.33786764705882355\n",
      "0.33786764705882355\n",
      "0.3386029411764706\n",
      "0.33933823529411766\n",
      "0.3400735294117647\n",
      "0.34044117647058825\n",
      "0.3408088235294118\n",
      "0.3411764705882353\n",
      "0.3411764705882353\n",
      "0.3411764705882353\n",
      "0.34154411764705883\n",
      "0.34191176470588236\n",
      "0.3422794117647059\n",
      "0.34301470588235294\n",
      "0.34375\n",
      "0.34411764705882353\n",
      "0.34448529411764706\n",
      "0.3448529411764706\n",
      "0.3452205882352941\n",
      "0.3463235294117647\n",
      "0.3466911764705882\n",
      "0.3477941176470588\n",
      "0.3488970588235294\n",
      "0.3488970588235294\n",
      "0.3492647058823529\n",
      "0.34963235294117645\n",
      "0.34963235294117645\n",
      "0.35\n",
      "0.35110294117647056\n",
      "0.35110294117647056\n",
      "0.3522058823529412\n",
      "0.35257352941176473\n",
      "0.3522058823529412\n",
      "0.3522058823529412\n",
      "0.3533088235294118\n",
      "0.3533088235294118\n",
      "0.3536764705882353\n",
      "0.35441176470588237\n",
      "0.35441176470588237\n",
      "0.3547794117647059\n",
      "0.3551470588235294\n",
      "0.35551470588235295\n",
      "0.35661764705882354\n",
      "0.35698529411764707\n",
      "0.35661764705882354\n",
      "0.35698529411764707\n",
      "0.35661764705882354\n",
      "0.35661764705882354\n",
      "0.35661764705882354\n",
      "0.35698529411764707\n",
      "0.35698529411764707\n",
      "0.35808823529411765\n",
      "0.3588235294117647\n",
      "0.3599264705882353\n",
      "0.3602941176470588\n",
      "0.36066176470588235\n",
      "0.3625\n",
      "0.3625\n",
      "0.3625\n",
      "0.36323529411764705\n",
      "0.3639705882352941\n",
      "0.3636029411764706\n",
      "0.36323529411764705\n",
      "0.3639705882352941\n",
      "0.36470588235294116\n",
      "0.36470588235294116\n",
      "0.36470588235294116\n",
      "0.36470588235294116\n",
      "0.3650735294117647\n",
      "0.3650735294117647\n",
      "0.3650735294117647\n",
      "0.3654411764705882\n",
      "0.36580882352941174\n",
      "0.3654411764705882\n",
      "0.3665441176470588\n",
      "0.3672794117647059\n",
      "0.36764705882352944\n",
      "0.36764705882352944\n",
      "0.36764705882352944\n",
      "0.3683823529411765\n",
      "0.36875\n",
      "0.36875\n",
      "0.36911764705882355\n",
      "0.3694852941176471\n",
      "0.36875\n",
      "0.3694852941176471\n",
      "0.3694852941176471\n",
      "0.36911764705882355\n",
      "0.36911764705882355\n",
      "0.3694852941176471\n",
      "0.3694852941176471\n",
      "0.3698529411764706\n",
      "0.3698529411764706\n",
      "0.3698529411764706\n",
      "0.37022058823529413\n",
      "0.37022058823529413\n",
      "0.37058823529411766\n",
      "0.37022058823529413\n",
      "0.37058823529411766\n",
      "0.37058823529411766\n",
      "0.37058823529411766\n",
      "0.3709558823529412\n",
      "0.3709558823529412\n",
      "0.3713235294117647\n",
      "0.37169117647058825\n",
      "0.3720588235294118\n",
      "0.3720588235294118\n",
      "0.37279411764705883\n",
      "0.37316176470588236\n",
      "0.3735294117647059\n",
      "0.3735294117647059\n",
      "0.3735294117647059\n",
      "0.3738970588235294\n",
      "0.3738970588235294\n",
      "0.37463235294117647\n",
      "0.37463235294117647\n",
      "0.375\n",
      "0.3764705882352941\n",
      "0.37720588235294117\n",
      "0.3779411764705882\n",
      "0.37830882352941175\n",
      "0.37830882352941175\n",
      "0.37830882352941175\n",
      "0.3790441176470588\n",
      "0.3790441176470588\n",
      "0.3790441176470588\n",
      "0.3790441176470588\n",
      "0.37941176470588234\n",
      "0.37977941176470587\n",
      "0.3801470588235294\n",
      "0.3801470588235294\n",
      "0.3805147058823529\n",
      "0.3801470588235294\n",
      "0.3801470588235294\n",
      "0.3801470588235294\n",
      "0.3801470588235294\n",
      "0.38088235294117645\n",
      "0.38088235294117645\n",
      "0.3816176470588235\n",
      "0.38198529411764703\n",
      "0.38198529411764703\n",
      "0.3827205882352941\n",
      "0.3830882352941177\n",
      "0.3834558823529412\n",
      "0.3830882352941177\n",
      "0.3830882352941177\n",
      "0.3834558823529412\n",
      "0.3834558823529412\n",
      "0.3834558823529412\n",
      "0.3834558823529412\n",
      "0.38382352941176473\n",
      "0.38382352941176473\n",
      "0.38382352941176473\n",
      "0.38419117647058826\n",
      "0.38419117647058826\n",
      "0.38529411764705884\n",
      "0.38529411764705884\n",
      "0.38566176470588237\n",
      "0.38566176470588237\n",
      "0.38566176470588237\n",
      "0.38566176470588237\n",
      "0.38566176470588237\n",
      "0.38566176470588237\n",
      "0.38566176470588237\n",
      "0.38566176470588237\n",
      "0.3860294117647059\n",
      "0.3860294117647059\n",
      "0.3863970588235294\n",
      "0.3863970588235294\n",
      "0.3863970588235294\n",
      "0.3863970588235294\n",
      "0.3860294117647059\n",
      "0.3871323529411765\n",
      "0.3875\n",
      "0.3875\n",
      "0.38786764705882354\n",
      "0.3886029411764706\n",
      "0.3897058823529412\n",
      "0.3900735294117647\n",
      "0.3900735294117647\n",
      "0.3900735294117647\n",
      "0.39080882352941176\n",
      "0.3915441176470588\n",
      "0.3915441176470588\n",
      "0.39191176470588235\n",
      "0.3915441176470588\n",
      "0.3915441176470588\n",
      "0.3922794117647059\n",
      "0.3922794117647059\n",
      "0.39338235294117646\n",
      "0.3941176470588235\n",
      "0.39448529411764705\n",
      "0.39448529411764705\n",
      "0.3952205882352941\n",
      "0.3952205882352941\n",
      "0.39558823529411763\n",
      "0.3948529411764706\n",
      "0.3948529411764706\n",
      "0.39448529411764705\n",
      "0.39448529411764705\n",
      "0.3948529411764706\n",
      "0.39448529411764705\n",
      "0.3952205882352941\n",
      "0.3952205882352941\n",
      "0.3963235294117647\n",
      "0.3966911764705882\n",
      "0.39705882352941174\n",
      "0.39705882352941174\n",
      "0.39705882352941174\n",
      "0.3977941176470588\n",
      "0.3985294117647059\n",
      "0.3985294117647059\n",
      "0.3985294117647059\n",
      "0.39889705882352944\n",
      "0.39926470588235297\n",
      "0.3996323529411765\n",
      "0.4\n",
      "0.40036764705882355\n",
      "0.40036764705882355\n",
      "0.4\n",
      "0.4007352941176471\n",
      "0.4011029411764706\n",
      "0.4011029411764706\n",
      "0.40147058823529413\n",
      "0.4022058823529412\n",
      "0.4025735294117647\n",
      "0.4033088235294118\n",
      "0.4036764705882353\n",
      "0.40404411764705883\n",
      "0.40441176470588236\n",
      "0.40441176470588236\n",
      "0.4047794117647059\n",
      "0.4051470588235294\n",
      "0.40551470588235294\n",
      "0.4051470588235294\n",
      "0.4051470588235294\n",
      "0.40625\n",
      "0.40661764705882353\n",
      "0.40661764705882353\n",
      "0.40661764705882353\n",
      "0.40625\n",
      "0.40625\n",
      "0.40698529411764706\n",
      "0.4073529411764706\n",
      "0.4077205882352941\n",
      "0.40808823529411764\n",
      "0.40845588235294117\n",
      "0.40808823529411764\n",
      "0.4077205882352941\n",
      "0.4077205882352941\n",
      "0.40808823529411764\n",
      "0.40808823529411764\n",
      "0.40845588235294117\n",
      "0.40955882352941175\n",
      "0.4099264705882353\n",
      "0.4102941176470588\n",
      "0.4113970588235294\n",
      "0.4117647058823529\n",
      "0.41213235294117645\n",
      "0.41213235294117645\n",
      "0.41213235294117645\n",
      "0.41213235294117645\n",
      "0.41323529411764703\n",
      "0.4139705882352941\n",
      "0.4139705882352941\n",
      "0.4139705882352941\n",
      "0.41360294117647056\n",
      "0.4139705882352941\n",
      "0.4147058823529412\n",
      "0.4147058823529412\n",
      "0.41507352941176473\n",
      "0.41544117647058826\n",
      "0.4158088235294118\n",
      "0.41507352941176473\n",
      "0.41507352941176473\n",
      "0.41507352941176473\n",
      "0.41544117647058826\n",
      "0.41507352941176473\n",
      "0.4147058823529412\n",
      "0.41507352941176473\n",
      "0.41544117647058826\n",
      "0.4158088235294118\n",
      "0.41654411764705884\n",
      "0.4161764705882353\n",
      "0.4161764705882353\n",
      "0.4161764705882353\n",
      "0.4172794117647059\n",
      "0.4176470588235294\n",
      "0.4176470588235294\n",
      "0.4183823529411765\n",
      "0.41801470588235295\n",
      "0.4183823529411765\n",
      "0.41875\n",
      "0.41911764705882354\n",
      "0.41911764705882354\n",
      "0.41875\n",
      "0.41875\n",
      "0.41801470588235295\n",
      "0.4183823529411765\n",
      "0.4183823529411765\n",
      "0.41801470588235295\n",
      "0.4183823529411765\n",
      "0.4183823529411765\n",
      "0.41911764705882354\n",
      "0.41948529411764707\n",
      "0.41948529411764707\n",
      "0.4202205882352941\n",
      "0.4209558823529412\n",
      "0.4213235294117647\n",
      "0.4209558823529412\n",
      "0.42169117647058824\n",
      "0.4213235294117647\n",
      "0.42169117647058824\n",
      "0.42205882352941176\n",
      "0.42205882352941176\n",
      "0.4224264705882353\n",
      "0.42205882352941176\n",
      "0.42205882352941176\n",
      "0.4224264705882353\n",
      "0.42316176470588235\n",
      "0.42316176470588235\n",
      "0.42316176470588235\n",
      "0.4235294117647059\n",
      "0.4235294117647059\n",
      "0.4235294117647059\n",
      "0.4238970588235294\n",
      "0.4238970588235294\n",
      "0.42426470588235293\n",
      "0.4238970588235294\n",
      "0.4238970588235294\n",
      "0.4238970588235294\n",
      "0.4238970588235294\n",
      "0.42463235294117646\n",
      "0.425\n",
      "0.42463235294117646\n",
      "0.42463235294117646\n",
      "0.4253676470588235\n",
      "0.4261029411764706\n",
      "0.4261029411764706\n",
      "0.4264705882352941\n",
      "0.42683823529411763\n",
      "0.42720588235294116\n",
      "0.42720588235294116\n",
      "0.4275735294117647\n",
      "0.4275735294117647\n",
      "0.4279411764705882\n",
      "0.42830882352941174\n",
      "0.42867647058823527\n",
      "0.4294117647058823\n",
      "0.4294117647058823\n",
      "0.4297794117647059\n",
      "0.4297794117647059\n",
      "0.43014705882352944\n",
      "0.43014705882352944\n",
      "0.43014705882352944\n",
      "0.4308823529411765\n",
      "0.43125\n",
      "0.43125\n",
      "0.43125\n",
      "0.43125\n",
      "0.43161764705882355\n",
      "0.43161764705882355\n",
      "0.43125\n",
      "0.43125\n",
      "0.43161764705882355\n",
      "0.43161764705882355\n",
      "0.43161764705882355\n",
      "0.4319852941176471\n",
      "0.43272058823529413\n",
      "0.43272058823529413\n",
      "0.43272058823529413\n",
      "0.43272058823529413\n",
      "0.43272058823529413\n",
      "0.43308823529411766\n",
      "0.4334558823529412\n",
      "0.4338235294117647\n",
      "0.4338235294117647\n",
      "0.43419117647058825\n",
      "0.4345588235294118\n",
      "0.4345588235294118\n",
      "0.43419117647058825\n",
      "0.43419117647058825\n",
      "0.43419117647058825\n",
      "0.43419117647058825\n",
      "0.4338235294117647\n",
      "0.4338235294117647\n",
      "0.4334558823529412\n",
      "0.43308823529411766\n",
      "0.4334558823529412\n",
      "0.4334558823529412\n",
      "0.4338235294117647\n",
      "0.4338235294117647\n",
      "0.43419117647058825\n",
      "0.43529411764705883\n",
      "0.4349264705882353\n",
      "0.4349264705882353\n",
      "0.43566176470588236\n",
      "0.43529411764705883\n",
      "0.43529411764705883\n",
      "0.43566176470588236\n",
      "0.43566176470588236\n",
      "0.4360294117647059\n",
      "0.4363970588235294\n",
      "0.4363970588235294\n",
      "0.4363970588235294\n",
      "0.43713235294117647\n",
      "0.43713235294117647\n",
      "0.43713235294117647\n",
      "0.43786764705882353\n",
      "0.4386029411764706\n",
      "0.4389705882352941\n",
      "0.43933823529411764\n",
      "0.43933823529411764\n",
      "0.43970588235294117\n",
      "0.43970588235294117\n",
      "0.4400735294117647\n",
      "0.4400735294117647\n",
      "0.4404411764705882\n",
      "0.4411764705882353\n",
      "0.4411764705882353\n",
      "0.4411764705882353\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "        self.w0 = np.random.randn(100, 784)\n",
    "        self.w1 = np.random.randn(5, 100)\n",
    "\n",
    "\n",
    "    def activation(self, x):\n",
    "        return expit(x)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        a0 = self.activation(self.w0 @ X.T)\n",
    "        pred = self.activation(self.w1 @ a0)\n",
    "\n",
    "        e1 = y.T - pred\n",
    "        e0 = e1.T @ self.w1\n",
    "\n",
    "        dw1 = e1 * pred * (1 - pred) @ a0.T / len(X)\n",
    "        dw0 = e0.T * a0 * (1 - a0) @ X / len(X)\n",
    "\n",
    "        assert dw1.shape == self.w1.shape\n",
    "        assert dw0.shape == self.w0.shape\n",
    "\n",
    "        self.w1 = self.w1 + self.lr * dw1\n",
    "        self.w0 = self.w0 + self.lr * dw0\n",
    "\n",
    "        # print(\"Kosten: \" + str(self.cost(pred, y)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        a0 = self.activation(self.w0 @ X.T)\n",
    "        pred = self.activation(self.w1 @ a0)\n",
    "        return pred\n",
    "\n",
    "    def cost(self, pred, y):\n",
    "        # SUM((y - pred)^2)\n",
    "        s = (1 / 2) * (y.T - pred) ** 2\n",
    "        return np.mean(np.sum(s, axis=0))\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "for i in range(0, 500):\n",
    "    for j in range(0, len(X_train), 100):\n",
    "        model.train(X_train[j:(j + 100), :] / 255., y_train_oh[j:(j + 100), :])\n",
    "\n",
    "    y_test_pred = model.predict(X_test / 255.)\n",
    "    y_test_pred = np.argmax(y_test_pred, axis=0)\n",
    "    print(np.mean(y_test_pred == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba2e879e-fc9e-463e-b068-026a7424b76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4411764705882353"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test_pred == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee28da-1f15-4af7-9be9-f5b8574ed567",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mehrere Ausgänge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c1e41c-3159-476d-9f75-bff398f0d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from numpy import load\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = load('../01_Dataset/dataset_28x28/X_train.npy').astype(np.float32).reshape(-1, 784)*1.0/255.0\n",
    "y_train = load('../01_Dataset/dataset_28x28/y_train.npy').astype(np.int32)\n",
    "\n",
    "X_test=load('../01_Dataset/dataset_28x28/X_test.npy').astype(np.float32).reshape(-1, 784)*1.0/255.0\n",
    "y_test=load('../01_Dataset/dataset_28x28/y_test.npy').astype(np.int32)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cdc4684-03b8-47e9-87f2-3b27fbd26d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7723e7b-abbb-4a81-a31c-ffdda7a6d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation=\"sigmoid\", input_shape=(784,)))\n",
    "model.add(Dense(5, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea312a7e-6706-445b-991e-a475abda2777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.4786 - accuracy: 0.8206\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4737 - accuracy: 0.8197\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4687 - accuracy: 0.8217\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.4638 - accuracy: 0.8224\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4586 - accuracy: 0.8263\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.4543 - accuracy: 0.8265\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4498 - accuracy: 0.8279\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.8290\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4417 - accuracy: 0.8269\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4374 - accuracy: 0.8304\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.4335 - accuracy: 0.8317\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4298 - accuracy: 0.8309\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.4264 - accuracy: 0.8323\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8328\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4192 - accuracy: 0.8359\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4156 - accuracy: 0.8369\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4124 - accuracy: 0.8369\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4092 - accuracy: 0.8384\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4061 - accuracy: 0.8397\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8389\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.4002 - accuracy: 0.8419\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3973 - accuracy: 0.8408\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3947 - accuracy: 0.8432\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3923 - accuracy: 0.8424\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3892 - accuracy: 0.8424\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3866 - accuracy: 0.8441\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3842 - accuracy: 0.8427\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3818 - accuracy: 0.8441\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3794 - accuracy: 0.8446\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3774 - accuracy: 0.8462\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3750 - accuracy: 0.8466\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3728 - accuracy: 0.8466\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3707 - accuracy: 0.8477\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8490\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3669 - accuracy: 0.8495\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8482\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3629 - accuracy: 0.8503\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3611 - accuracy: 0.8504\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8488\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3574 - accuracy: 0.8506\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3557 - accuracy: 0.8507\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3539 - accuracy: 0.8515\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3527 - accuracy: 0.8510\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.8506\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.8517\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3476 - accuracy: 0.8528\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3461 - accuracy: 0.8540\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3446 - accuracy: 0.8521\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.3436 - accuracy: 0.8551\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3422 - accuracy: 0.8540\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3406 - accuracy: 0.8515\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3393 - accuracy: 0.8540\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3378 - accuracy: 0.8542\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3370 - accuracy: 0.8550\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3354 - accuracy: 0.8550\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.8570\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.8586\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8566\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8577\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3296 - accuracy: 0.8539\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3286 - accuracy: 0.8583\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8586\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8566\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8591\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8584\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8572\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3225 - accuracy: 0.8594\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3217 - accuracy: 0.8561\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3205 - accuracy: 0.8592\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3199 - accuracy: 0.8597\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3190 - accuracy: 0.8597\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3178 - accuracy: 0.8597\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3171 - accuracy: 0.8600\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8588\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8591\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3147 - accuracy: 0.8596\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3136 - accuracy: 0.8578\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3130 - accuracy: 0.8602\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3125 - accuracy: 0.8594\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3115 - accuracy: 0.8597\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3109 - accuracy: 0.8611\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3099 - accuracy: 0.8599\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3094 - accuracy: 0.8614\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3085 - accuracy: 0.8630\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3079 - accuracy: 0.8610\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3071 - accuracy: 0.8599\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3064 - accuracy: 0.8619\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3060 - accuracy: 0.8596\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3053 - accuracy: 0.8627\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3043 - accuracy: 0.8637\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3037 - accuracy: 0.8624\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3035 - accuracy: 0.8627\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3027 - accuracy: 0.8605\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3020 - accuracy: 0.8624\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3016 - accuracy: 0.8622\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3008 - accuracy: 0.8635\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.3002 - accuracy: 0.8613\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2997 - accuracy: 0.8618\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8643\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.2984 - accuracy: 0.8629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23939ddb348>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678d636-36c5-4a65-9ef0-ccfa5a358253",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test.reshape(-1, 784), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bcf274-5871-468c-a53f-b9e08ecd6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test.reshape(-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838d57d-e600-42d1-b817-3f781484d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(y_test[1])\n",
    "\n",
    "plt.imshow(X_test[1].reshape(28,28), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a75354-8a08-47e0-8cd7-0106b54b6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test.reshape(-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd3e6d-405f-4dcf-8afe-085561910aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(pred[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aeb13b-c957-4616-8e2c-56ad32f2a3f9",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798ad83-d8e8-42aa-b1b0-d253ac264270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ytrue = pd.Series(np.argmax(y_test, axis= 1), name = 'ytrue')\n",
    "ypred = pd.Series(np.argmax(pred, axis= 1), name = 'pred')\n",
    "pd.crosstab(ytrue, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1493c26-bcf7-459d-97c8-749051c21e9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lernkurve plotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab874b-e87e-44c9-a3aa-a78250e379fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, lr = 0.1):\n",
    "        self.lr = lr\n",
    "\n",
    "        self.w0 = np.random.randn(100, 784)\n",
    "        self.w1 = np.random.randn(5, 100)\n",
    "\n",
    "\n",
    "    def activation(self, x):\n",
    "        return expit(x)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        a0 = self.activation(self.w0 @ X.T)\n",
    "        pred = self.activation(self.w1 @ a0)\n",
    "\n",
    "        e1 = y.T - pred\n",
    "        e0 = e1.T @ self.w1\n",
    "\n",
    "        dw1 = e1 * pred * (1 - pred) @ a0.T / len(X)\n",
    "        dw0 = e0.T * a0 * (1 - a0) @ X / len(X)\n",
    "\n",
    "        assert dw1.shape == self.w1.shape\n",
    "        assert dw0.shape == self.w0.shape\n",
    "\n",
    "        self.w1 = self.w1 + self.lr * dw1\n",
    "        self.w0 = self.w0 + self.lr * dw0\n",
    "\n",
    "        # print(\"Kosten: \" + str(self.cost(pred, y)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        a0 = self.activation(self.w0 @ X.T)\n",
    "        pred = self.activation(self.w1 @ a0)\n",
    "        return pred\n",
    "\n",
    "    def cost(self, pred, y):\n",
    "        # SUM((y - pred)^2)\n",
    "        s = (1 / 2) * (y.T - pred) ** 2\n",
    "        return np.mean(np.sum(s, axis=0))\n",
    "\n",
    "limits = [100, 1000, 3000, 9000, 10500]\n",
    "test_accs = []\n",
    "train_accs = []\n",
    "for limit in limits:\n",
    "    model = NeuralNetwork(0.25)\n",
    "\n",
    "    for i in range(0, 100):\n",
    "        for j in range(0, limit, 100):\n",
    "           model.train(X_train[j:(j + 100), :] / 255., y_train_oh[j:(j + 100), :])\n",
    "\n",
    "\n",
    "    y_test_pred = model.predict(X_test / 255.)\n",
    "    y_test_pred = np.argmax(y_test_pred, axis=0)\n",
    "    test_acc = np.mean(y_test_pred == y_test)\n",
    "\n",
    "    y_train_pred = model.predict(X_train / 255.)\n",
    "    y_train_pred = np.argmax(y_train_pred, axis=0)\n",
    "    train_acc = np.mean(y_train_pred == y_train)\n",
    "\n",
    "    test_accs.append(test_acc)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(limits, train_accs, label=\"Training\")\n",
    "plt.plot(limits, test_accs, label=\"Test\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659348c4-222c-48a9-ac03-1f679fe47c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b43b0a-a629-4314-9afe-824d4892292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13167647-efc2-4aa4-89b6-8a39874c8ca9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lernrate plotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c7e65-2a20-4f9d-9d07-aa254422c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, lr = 0.1):\n",
    "        self.lr = lr\n",
    "\n",
    "        self.w0 = np.random.randn(100, 784)\n",
    "        self.w1 = np.random.randn(5, 100)\n",
    "\n",
    "\n",
    "    def activation(self, x):\n",
    "        return expit(x)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        a0 = self.activation(self.w0 @ X.T)\n",
    "        pred = self.activation(self.w1 @ a0)\n",
    "\n",
    "        e1 = y.T - pred\n",
    "        e0 = e1.T @ self.w1\n",
    "\n",
    "        dw1 = e1 * pred * (1 - pred) @ a0.T / len(X)\n",
    "        dw0 = e0.T * a0 * (1 - a0) @ X / len(X)\n",
    "\n",
    "        assert dw1.shape == self.w1.shape\n",
    "        assert dw0.shape == self.w0.shape\n",
    "\n",
    "        self.w1 = self.w1 + self.lr * dw1\n",
    "        self.w0 = self.w0 + self.lr * dw0\n",
    "\n",
    "        # print(\"Kosten: \" + str(self.cost(pred, y)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        a0 = self.activation(self.w0 @ X.T)\n",
    "        pred = self.activation(self.w1 @ a0)\n",
    "        return pred\n",
    "\n",
    "    def cost(self, pred, y):\n",
    "        # SUM((y - pred)^2)\n",
    "        s = (1 / 2) * (y.T - pred) ** 2\n",
    "        return np.mean(np.sum(s, axis=0))\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "epochs = []\n",
    "costs = []\n",
    "accs = []\n",
    "\n",
    "for i in range(0, 50):\n",
    "    for j in range(0, 10500, 100):\n",
    "        model.train(X_train[j:(j + 100), :] / 255., y_train_oh[j:(j + 100), :])\n",
    "\n",
    "    cost = model.cost(model.predict(X_train), y_train_oh)\n",
    "\n",
    "    y_test_pred = model.predict(X_test / 255.)\n",
    "    y_test_pred = np.argmax(y_test_pred, axis=0)\n",
    "    acc = np.mean(y_test_pred == y_test)\n",
    "\n",
    "    epochs.append(i + 1)\n",
    "    costs.append(cost)\n",
    "    accs.append(acc)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(epochs, costs, label=\"Kosten\")\n",
    "plt.plot(epochs, accs, label=\"Genauigkeit\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d39364-d08d-416e-a7b0-decab5a28071",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = np.mean(y_test_pred == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3d12db-fe89-4e9b-b113-c288b426a7ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Netzwerkgröße"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74d34e-1324-446b-8b8f-6d1f5167b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, lr = 0.1, hidden_size = 100):\n",
    "        self.lr = lr\n",
    "\n",
    "        self.w0 = np.random.randn(hidden_size, 784)\n",
    "        self.w1 = np.random.randn(5, hidden_size)\n",
    "\n",
    "\n",
    "    def activation(self, x):\n",
    "        return expit(x)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        a0 = self.activation(self.w0 @ X.T)\n",
    "        pred = self.activation(self.w1 @ a0)\n",
    "\n",
    "        e1 = y.T - pred\n",
    "        e0 = e1.T @ self.w1\n",
    "\n",
    "        dw1 = e1 * pred * (1 - pred) @ a0.T / len(X)\n",
    "        dw0 = e0.T * a0 * (1 - a0) @ X / len(X)\n",
    "\n",
    "        assert dw1.shape == self.w1.shape\n",
    "        assert dw0.shape == self.w0.shape\n",
    "\n",
    "        self.w1 = self.w1 + self.lr * dw1\n",
    "        self.w0 = self.w0 + self.lr * dw0\n",
    "\n",
    "        # print(\"Kosten: \" + str(self.cost(pred, y)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        a0 = self.activation(self.w0 @ X.T)\n",
    "        pred = self.activation(self.w1 @ a0)\n",
    "        return pred\n",
    "\n",
    "    def cost(self, pred, y):\n",
    "        # SUM((y - pred)^2)\n",
    "        s = (1 / 2) * (y.T - pred) ** 2\n",
    "        return np.mean(np.sum(s, axis=0))\n",
    "\n",
    "for hidden_size in [500, 600, 700, 800]:\n",
    "\n",
    "    model = NeuralNetwork(0.3, hidden_size)\n",
    "\n",
    "    for i in range(0, 25):\n",
    "        for j in range(0, 10500, 100):\n",
    "            model.train(X_train[j:(j + 100), :] / 255., y_train_oh[j:(j + 100), :])\n",
    "\n",
    "        # cost = model.cost(model.predict(X_train), y_train_oh)\n",
    "\n",
    "    y_test_pred = model.predict(X_test / 255.)\n",
    "    y_test_pred = np.argmax(y_test_pred, axis=0)\n",
    "    acc = np.mean(y_test_pred == y_test)\n",
    "\n",
    "    print(str(hidden_size) + \": \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0987adc-cc91-4ec7-9091-ed042449438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in range(0, len(X_test)):\n",
    "    if y_test_pred[i] == 2 and y_test[i] ==1:\n",
    "        count += 1\n",
    "        plt.imshow(X_test[i].reshape(28, 28))\n",
    "        plt.show()\n",
    "        print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
