{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108421eb-0adc-4fb1-949b-215ca2692466",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3394496e-8969-4515-850d-aee7b2f5da81",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a141745-ce12-4fbe-ac36-2a52cc7235a8",
   "metadata": {},
   "source": [
    "Mit Hilfe eines CNN-Layer bekommt das neuronale Netz ein \"Verständnis\" für Bilder \"eingebaut\". Das NN ist somit auf die Erkennung von Bildern spezialisiert und demensprechend Leistungsfähiger als ein NN ohne dieses Bildverständnis.\n",
    "\n",
    "- Kantenerkennung\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8816b-144f-4b32-ad45-f4d6683fceef",
   "metadata": {},
   "source": [
    "Das CNN besitzt gegenüber dem neuronalem Netz eine Intuition darüber was ein Bild ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653609a-4bf2-449d-93b4-04d605513e02",
   "metadata": {},
   "source": [
    "Das Neuronale Netz kann auf der ersten Ebene lernen, Kanten zu erkennen. Diese Ebene ist dann für die Kantenerkennung zuständig. Kante ist Kante egal wo auf dem Bild. Diese \"Features\" werden in den nachfolge Schichten weiterverarbeitet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a9dad-b448-4da3-a864-3508e7a436da",
   "metadata": {},
   "source": [
    "### Beispiel einer einfachen Convolution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67447c0f-92af-49ba-a955-fa114b5a3f31",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/image-processing-with-python-convolutional-filters-and-kernels-b9884d91a8fd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf2941-2fef-4533-b380-8364ee122bb8",
   "metadata": {},
   "source": [
    "Die Filter oder Kernels gibt man nicht vor sondern lässt die Werte vom Convolutional Layer ermitteln. Die Kernels werden dabei so bestimmt dass sie für das Problem am meisten Sinn machen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4b3f2-0ab0-4081-aad9-db6ca6de6095",
   "metadata": {},
   "source": [
    "Wir möchten nicht nur vertikale Kanten finden, sondern auch schräge und waagerechte. Da jeder Filter für ein bestimmtes Feature zuständig ist, benötigt das CNN mehrere solcher Filter um alle relevanten Zusammenhänge extrahieren zu können. Die Anzahl an Filter die wir bereitstellen hängt von den Daten ab und ist ein Hyperparameter den man tunen muss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8499cc-7e86-4121-bc5e-ef990dae7c06",
   "metadata": {},
   "source": [
    "## CNN mit Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3dc04-1fef-4759-b1eb-7d7639dfa2c2",
   "metadata": {},
   "source": [
    "Wir wollen nun ein CNN mit Keras entwickeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3c6bb73-b765-4ffd-9ad8-bd40d6c856f4",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 02:23:30.639376: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-23 02:23:30.639401: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Vorstellung: MNIST-Daten!\n",
    "# http://yann.lecun.com/exdb/mnist/\n",
    "# FashionMNIST: https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "X_train = load('../02_NN/Dataset/X_train.npy').astype(np.float32)#.reshape(-1, 784)\n",
    "y_train = load('../02_NN/Dataset/y_train.npy')\n",
    "\n",
    "\n",
    "#oh = OneHotEncoder()\n",
    "#y_train_oh = oh.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "\n",
    "X_test=load('../02_NN/Dataset/X_test.npy').astype(np.float32)#.reshape(-1, 784)\n",
    "y_test=load('../02_NN/Dataset/y_test.npy')\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115126e4-5ced-4650-81cc-88f3532b12c6",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca085f39-06ab-410d-997f-32e53a43fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd6231-4811-4b93-92d4-306a0a566b02",
   "metadata": {},
   "source": [
    "Das Format der Daten passt noch nicht zum geforderten Eingangsformat.\n",
    "Das CNN verlangt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3824b2-9891-4f56-a424-354ddd10833b",
   "metadata": {},
   "source": [
    "Bei einem Wert am Ausgang zwischen 0 und 1 verwendet man \"binary crossentropy\". Hat man mehrere Werte / Kategorien am Ausgang, dann verwendet man categorical crossentropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd8b25-6c51-4e1e-b1ec-c54c9c052c60",
   "metadata": {},
   "source": [
    "## stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c55a2fb7-2f55-4978-9cf1-ac3e83d1a5f2",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - 1s 22ms/step - loss: 229.5953 - accuracy: 0.3178\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.9440 - accuracy: 0.6190\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.6530 - accuracy: 0.7649\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.5013 - accuracy: 0.8199\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4058 - accuracy: 0.8393\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4397 - accuracy: 0.8294\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3674 - accuracy: 0.8583\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3386 - accuracy: 0.8624\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3027 - accuracy: 0.8836\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3147 - accuracy: 0.8754\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2679 - accuracy: 0.8914\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2448 - accuracy: 0.9017\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2369 - accuracy: 0.9060\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2150 - accuracy: 0.9114\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3123 - accuracy: 0.8828\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2253 - accuracy: 0.9091\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1853 - accuracy: 0.9243\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1935 - accuracy: 0.9216\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1948 - accuracy: 0.9217\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1432 - accuracy: 0.9435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f180005d790>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN!\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "#model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train.reshape(10500,28,28,1),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0406b34-52b7-435d-a0eb-1f51c57c4042",
   "metadata": {},
   "source": [
    "## rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53a40d7c-2016-42a4-81fe-cfd4b82db2cd",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - 1s 22ms/step - loss: 242.0397 - accuracy: 0.4830\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 63.1420 - accuracy: 0.6419\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 27.1203 - accuracy: 0.7310\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 10.0603 - accuracy: 0.8360\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 4.6834 - accuracy: 0.8353\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 2.2660 - accuracy: 0.8735\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 1.4034 - accuracy: 0.9078\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.9845 - accuracy: 0.9339\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.5971 - accuracy: 0.9523\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0607 - accuracy: 0.9875\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4934 - accuracy: 0.9664\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0084 - accuracy: 0.9981\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.6108 - accuracy: 0.9581\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 3.7297e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4393 - accuracy: 0.9813\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 5.3599e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 2.5452e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3049 - accuracy: 0.9837\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 4.4494e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17e27e5e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN!\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "#model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train.reshape(10500,28,28,1),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4e229-0dca-4a8c-9912-556f062f2a4b",
   "metadata": {},
   "source": [
    "## Two Conv2D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b708e2e1-db14-424d-8f69-059a20aa0044",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - 2s 96ms/step - loss: 21.6524 - accuracy: 0.6189\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - 2s 95ms/step - loss: 0.8093 - accuracy: 0.8665\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - 2s 95ms/step - loss: 0.4227 - accuracy: 0.8997\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - 2s 94ms/step - loss: 0.1646 - accuracy: 0.9585\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - 2s 98ms/step - loss: 0.0131 - accuracy: 0.9990\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - 2s 96ms/step - loss: 0.2447 - accuracy: 0.9744\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - 2s 96ms/step - loss: 0.0067 - accuracy: 0.9998\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - 2s 95ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - 2s 118ms/step - loss: 0.0815 - accuracy: 0.9895\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - 3s 127ms/step - loss: 0.0597 - accuracy: 0.9820\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - 3s 127ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - 3s 126ms/step - loss: 3.9080e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - 3s 128ms/step - loss: 1.1759e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - 3s 127ms/step - loss: 0.0034 - accuracy: 0.9983\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - 3s 126ms/step - loss: 0.1700 - accuracy: 0.9794\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - 3s 125ms/step - loss: 2.5466e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - 3s 128ms/step - loss: 1.7362e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - 3s 132ms/step - loss: 6.7044e-05 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - 3s 128ms/step - loss: 7.7813e-05 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - 3s 132ms/step - loss: 0.0971 - accuracy: 0.9824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17e27a5150>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN!\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train.reshape(10500,28,28,1),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d690a28-a9e8-4693-bea8-369b9e383574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
