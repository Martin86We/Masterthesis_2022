{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108421eb-0adc-4fb1-949b-215ca2692466",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eec8b4-51d0-4c5e-b5e5-13756c5d3f69",
   "metadata": {},
   "source": [
    "Ein Theorem aus dem Jahr 1988, das \"Universal Approximation Theorem\", sagt, dass jede beliebige, glatte Funktion, durch ein NN mit nur einem Hidden Layer approximiert werden kann. Nach diesem Theorem nach, würde dieses einfache NN bereits in der Lage sein jedes beliebige Bild bzw. die Funktion der Pixelwerte zu erlernen. Die Fehler und die lange Rechenzeit zeigen die Probleme in der Praxis. Denn um dieses Theorem zu erfüllen sind für sehr einfache Netze unendlich viel Rechenleistung, Zeit und Trainingsbeispiele nötig. Diese stehen i.d.R. nicht zur Verfügung. Für die Bilderkennung haben sich CNN's als sehr wirksam erwiesen. Die Arbeitsweise soll in diesem Abschnitt erläutert werden.\n",
    "Der Grundgedanke bei der Nutzung der Convolutional Layer ist, dem NN zusätzliches \"Spezialwissen\" über die Daten zu geben. Das NN ist durch den zusätzlichen Convolutional Layer in der Lage, spezielle Bildelemente und Strukturen besser zu erkennen. \n",
    "\n",
    "Es werden meist mehrere Convolutional Layer hintereinander geschalten. Das NN kann auf der ersten Ebene lernen, Kanten zu erkennen. Auf weiteren Ebenen lernt es dann weitere \"Bild-Features\" wie z.B. Übergänge, Rundungen o.ä. zu erkennen. Diese werden auf höheren Ebenen weiterverarbeitet.  \n",
    "\n",
    "**Beispiel einer einfachen 1D-Faltung:**\n",
    "\n",
    "Die beiden einfachen Beispiele sollen die Berechnung verdeutlichen. Die Filterfunktion wird auf die Pixel gelegt und Elementweise multipliziert. \n",
    "Im folgenden Beispiel werden 3 Pixel eines Bildes verwendet. Die Ergebnisse sagen etwas über den Bildinhalt aus:\n",
    "\n",
    "- positives Ergebnis: Übergang von hell zu dunkel   \n",
    "- negatives Ergebnis: Übergang von dunkel nach hell\n",
    "- neutrales Ergebnis: Übergang wechselnd, hell-dunkel-hell  oder dunkel-hell-dunkel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93daa70-441a-4fa0-967b-6b391f0fc334",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"cnn_1d.png\" alt=\"pozi\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Eindimensionale Faltung\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b285084-c322-4486-89a5-1b51c6a210cc",
   "metadata": {},
   "source": [
    "Da ein Bild aus mehr als 3 Pixel besteht, muss die Filterfunktion über das gesamte Bild \"geschoben\" werden. Das folgende Beispiel demonstriert den Vorgang der Convolution im Fall eines eindimensionalen Filter. Der Filter besteht in diesem Fall wieder aus einem Zeilenvektor mit 3 Elementen. Der Filter wird nun Pixelweise über die Bildzeile geschoben, die Ergebnisse werden gespeichert und geben wiederum Aufschluss über die Bildstruktur.\n",
    "Die Ergebnisse zeigen wieder die enthaltene Bildstruktur: \n",
    "\n",
    "- 1: hell-dunkel\n",
    "- 0: hell-dunkel-hell\n",
    "- 0: dunkel-hell-dunkel\n",
    "- 1: hell-dunkel\n",
    "--1: dunkel-hell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2a353-264e-4cd0-b0cd-570fd13d8849",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"cnn_1d_long.png\" alt=\"pozi\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Eindimensionale Faltung mit mehreren Übergängen\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b1c32-d7bc-4295-af74-724dd197cc68",
   "metadata": {},
   "source": [
    "## 2-Dimensionale Faltung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a59ef7-1831-4f2e-bf80-b1024bc8be98",
   "metadata": {},
   "source": [
    "In der Praxis werden in der Bilderkennung 2-dimensionale Filter verwendet, ein häufig verwendetes Format ist ein 3x3 Filter. Der Vorgang ist analog zum eindimensionalen Fall, der Filter wird über das gesamte Bild geschoben. Das folgende Beispiel zeigt einen Filter, der in der Lage ist, senkrechte Kanten zu erkennen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19423a6-4d69-4eea-9e8d-cb39263c9fa0",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"cnn_2d_a.png\" alt=\"pozi\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Eindimensionale Faltung mit mehreren Übergängen\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c7da6-b46c-40ef-90f1-9d9288aed393",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"cnn_2d_b.png\" alt=\"pozi\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Eindimensionale Faltung mit mehreren Übergängen\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4663d5-ada5-4a3c-b1c1-3bbe9df35264",
   "metadata": {},
   "source": [
    "Die Werte der Filter bilden die Gewichte des Convolutional Layer. Diese Gewichte werden durch das Training selbst bestimmt und somit ist das CNN in der Lage, sich selbstständig auf relevante Features zu fokussieren. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd499995-ca95-43b7-9a67-4fdc64012e5b",
   "metadata": {},
   "source": [
    "**Im folgenden noch weitere Ergebnisse für bestimmte Bildstrukturen:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77052593-11b7-48fc-a91a-e43b7ed21e0a",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"cnn_2d_c.png\" alt=\"pozi\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Eindimensionale Faltung mit mehreren Übergängen\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff5b0b-6aa6-4dc5-8089-dd5ec96e8df0",
   "metadata": {},
   "source": [
    ":::{figure-md} markdown-fig\n",
    "<img src=\"cnn_2d_d.png\" alt=\"pozi\" class=\"bg-primary mb-1\" width=\"900px\">\n",
    "\n",
    "Eindimensionale Faltung mit mehreren Übergängen\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3394496e-8969-4515-850d-aee7b2f5da81",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a141745-ce12-4fbe-ac36-2a52cc7235a8",
   "metadata": {},
   "source": [
    "Mit Hilfe eines CNN-Layer bekommt das neuronale Netz ein \"Verständnis\" für Bilder \"eingebaut\". Das NN ist somit auf die Erkennung von Bildern spezialisiert und demensprechend Leistungsfähiger als ein NN ohne dieses Bildverständnis.\n",
    "\n",
    "- Kantenerkennung\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8816b-144f-4b32-ad45-f4d6683fceef",
   "metadata": {},
   "source": [
    "Das CNN besitzt gegenüber dem neuronalem Netz eine Intuition darüber was ein Bild ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653609a-4bf2-449d-93b4-04d605513e02",
   "metadata": {},
   "source": [
    "Das Neuronale Netz kann auf der ersten Ebene lernen, Kanten zu erkennen. Diese Ebene ist dann für die Kantenerkennung zuständig. Kante ist Kante egal wo auf dem Bild. Diese \"Features\" werden in den nachfolge Schichten weiterverarbeitet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a9dad-b448-4da3-a864-3508e7a436da",
   "metadata": {},
   "source": [
    "### Beispiel einer einfachen Convolution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67447c0f-92af-49ba-a955-fa114b5a3f31",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/image-processing-with-python-convolutional-filters-and-kernels-b9884d91a8fd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf2941-2fef-4533-b380-8364ee122bb8",
   "metadata": {},
   "source": [
    "Die Filter oder Kernels gibt man nicht vor sondern lässt die Werte vom Convolutional Layer ermitteln. Die Kernels werden dabei so bestimmt dass sie für das Problem am meisten Sinn machen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4b3f2-0ab0-4081-aad9-db6ca6de6095",
   "metadata": {},
   "source": [
    "Wir möchten nicht nur vertikale Kanten finden, sondern auch schräge und waagerechte. Da jeder Filter für ein bestimmtes Feature zuständig ist, benötigt das CNN mehrere solcher Filter um alle relevanten Zusammenhänge extrahieren zu können. Die Anzahl an Filter die wir bereitstellen hängt von den Daten ab und ist ein Hyperparameter den man tunen muss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8499cc-7e86-4121-bc5e-ef990dae7c06",
   "metadata": {},
   "source": [
    "## CNN mit Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3dc04-1fef-4759-b1eb-7d7639dfa2c2",
   "metadata": {},
   "source": [
    "Wir wollen nun ein CNN mit Keras entwickeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3c6bb73-b765-4ffd-9ad8-bd40d6c856f4",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 02:23:30.639376: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-23 02:23:30.639401: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Vorstellung: MNIST-Daten!\n",
    "# http://yann.lecun.com/exdb/mnist/\n",
    "# FashionMNIST: https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "X_train = load('../02_NN/Dataset/X_train.npy').astype(np.float32)#.reshape(-1, 784)\n",
    "y_train = load('../02_NN/Dataset/y_train.npy')\n",
    "\n",
    "\n",
    "#oh = OneHotEncoder()\n",
    "#y_train_oh = oh.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "\n",
    "X_test=load('../02_NN/Dataset/X_test.npy').astype(np.float32)#.reshape(-1, 784)\n",
    "y_test=load('../02_NN/Dataset/y_test.npy')\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115126e4-5ced-4650-81cc-88f3532b12c6",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca085f39-06ab-410d-997f-32e53a43fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd6231-4811-4b93-92d4-306a0a566b02",
   "metadata": {},
   "source": [
    "Das Format der Daten passt noch nicht zum geforderten Eingangsformat.\n",
    "Das CNN verlangt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3824b2-9891-4f56-a424-354ddd10833b",
   "metadata": {},
   "source": [
    "Bei einem Wert am Ausgang zwischen 0 und 1 verwendet man \"binary crossentropy\". Hat man mehrere Werte / Kategorien am Ausgang, dann verwendet man categorical crossentropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd8b25-6c51-4e1e-b1ec-c54c9c052c60",
   "metadata": {},
   "source": [
    "## stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c55a2fb7-2f55-4978-9cf1-ac3e83d1a5f2",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - 1s 22ms/step - loss: 229.5953 - accuracy: 0.3178\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.9440 - accuracy: 0.6190\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.6530 - accuracy: 0.7649\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.5013 - accuracy: 0.8199\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4058 - accuracy: 0.8393\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4397 - accuracy: 0.8294\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3674 - accuracy: 0.8583\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3386 - accuracy: 0.8624\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3027 - accuracy: 0.8836\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - 0s 24ms/step - loss: 0.3147 - accuracy: 0.8754\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2679 - accuracy: 0.8914\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2448 - accuracy: 0.9017\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2369 - accuracy: 0.9060\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2150 - accuracy: 0.9114\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3123 - accuracy: 0.8828\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2253 - accuracy: 0.9091\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1853 - accuracy: 0.9243\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1935 - accuracy: 0.9216\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1948 - accuracy: 0.9217\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.1432 - accuracy: 0.9435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f180005d790>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN!\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "#model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train.reshape(10500,28,28,1),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0406b34-52b7-435d-a0eb-1f51c57c4042",
   "metadata": {},
   "source": [
    "## rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53a40d7c-2016-42a4-81fe-cfd4b82db2cd",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - 1s 22ms/step - loss: 242.0397 - accuracy: 0.4830\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 63.1420 - accuracy: 0.6419\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 27.1203 - accuracy: 0.7310\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 10.0603 - accuracy: 0.8360\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 4.6834 - accuracy: 0.8353\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 2.2660 - accuracy: 0.8735\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 1.4034 - accuracy: 0.9078\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.9845 - accuracy: 0.9339\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.5971 - accuracy: 0.9523\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0607 - accuracy: 0.9875\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4934 - accuracy: 0.9664\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0084 - accuracy: 0.9981\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.6108 - accuracy: 0.9581\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 3.7297e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.4393 - accuracy: 0.9813\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 5.3599e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 2.5452e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.3049 - accuracy: 0.9837\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 4.4494e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17e27e5e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN!\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "#model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train.reshape(10500,28,28,1),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4e229-0dca-4a8c-9912-556f062f2a4b",
   "metadata": {},
   "source": [
    "## Two Conv2D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b708e2e1-db14-424d-8f69-059a20aa0044",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "21/21 [==============================] - 2s 96ms/step - loss: 21.6524 - accuracy: 0.6189\n",
      "Epoch 2/20\n",
      "21/21 [==============================] - 2s 95ms/step - loss: 0.8093 - accuracy: 0.8665\n",
      "Epoch 3/20\n",
      "21/21 [==============================] - 2s 95ms/step - loss: 0.4227 - accuracy: 0.8997\n",
      "Epoch 4/20\n",
      "21/21 [==============================] - 2s 94ms/step - loss: 0.1646 - accuracy: 0.9585\n",
      "Epoch 5/20\n",
      "21/21 [==============================] - 2s 98ms/step - loss: 0.0131 - accuracy: 0.9990\n",
      "Epoch 6/20\n",
      "21/21 [==============================] - 2s 96ms/step - loss: 0.2447 - accuracy: 0.9744\n",
      "Epoch 7/20\n",
      "21/21 [==============================] - 2s 96ms/step - loss: 0.0067 - accuracy: 0.9998\n",
      "Epoch 8/20\n",
      "21/21 [==============================] - 2s 95ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "21/21 [==============================] - 2s 118ms/step - loss: 0.0815 - accuracy: 0.9895\n",
      "Epoch 10/20\n",
      "21/21 [==============================] - 3s 127ms/step - loss: 0.0597 - accuracy: 0.9820\n",
      "Epoch 11/20\n",
      "21/21 [==============================] - 3s 127ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "21/21 [==============================] - 3s 126ms/step - loss: 3.9080e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "21/21 [==============================] - 3s 128ms/step - loss: 1.1759e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "21/21 [==============================] - 3s 127ms/step - loss: 0.0034 - accuracy: 0.9983\n",
      "Epoch 15/20\n",
      "21/21 [==============================] - 3s 126ms/step - loss: 0.1700 - accuracy: 0.9794\n",
      "Epoch 16/20\n",
      "21/21 [==============================] - 3s 125ms/step - loss: 2.5466e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "21/21 [==============================] - 3s 128ms/step - loss: 1.7362e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "21/21 [==============================] - 3s 132ms/step - loss: 6.7044e-05 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "21/21 [==============================] - 3s 128ms/step - loss: 7.7813e-05 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "21/21 [==============================] - 3s 132ms/step - loss: 0.0971 - accuracy: 0.9824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17e27a5150>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN!\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train.reshape(10500,28,28,1),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d690a28-a9e8-4693-bea8-369b9e383574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
